{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on the Tweets for the Celebrities Born Today\n",
    "\n",
    "### Final Project - Mastering Python\n",
    "### Author: Sammy Yu\n",
    "### Date: May 30, 2016\n",
    "\n",
    "### I Introduction\n",
    "The [IMDB](http://m.imdb.com/feature/bornondate) web site posts 10 celebrities whose birthday is today everyday. For each celebrity in the list, four pieces of information are shown, namely, a small photo of the celebrity, his/her name, profession, such as \"actor\" and one's best work, e.g. a movie.\n",
    "\n",
    "The project extracts those data from the web site using web scraping. Once the data is extracted, they are passed to a different module for a sentiment analysis on the tweets posted by the twitter users about those celebrities.\n",
    "\n",
    "The sentiment analysis outputs the overall analysis for each celebrity in four possible results: \"POSITIVE\", \"NEGATIVE\", \"SEMI-POSITIVE\" or \"NEUTRAL\". \"POSITIVE\" means the celebrity receives mostly good words about him/her from among the twitter users. \"NEGATIVE\" means the celebrity does not receive much good words from the twitter users. \"SEMI-POSTIVE\" means some good words from the twitter users but not as much as the positive one. \"NEUTRAL\" means the celebrity receives around 50% positive and 50% negative words from the twitter users.\n",
    "\n",
    "### II Problem Statement\n",
    "Build an application to pull the data of 10 celebrities who were born today from the IMDB web site, and then pull streaming data from twitter with taking the following input from the user:\n",
    "* Total number of tweets to be pulled for all 10 celebrities. If the number is reached, the application should terminate itself.\n",
    "* Once tweets are fetched, we need to find sentiment for each celebrity's name and finally compare them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III Solution Flow Diagram\n",
    "\n",
    "![](./Flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV Tools and Packages Used\n",
    "\n",
    "The application is written in Python 2.7x. The tools and packages used are listed as below:\n",
    "\n",
    "#### Tools:\n",
    "* PyCharm Community Edition\n",
    "* Jupyter Notebook\n",
    "\n",
    "#### Packages:\n",
    "* BeautifulSoup\n",
    "* Selenium\n",
    "* Tweepy\n",
    "* re\n",
    "* matplotlib\n",
    "* sys\n",
    "* codecs\n",
    "* csv\n",
    "* string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V Solution: Python Script with proper commenting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import required packages\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import sys\n",
    "import codecs\n",
    "import csv\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## class used to scrape the celebrity data from the IMDB web site\n",
    "class imdb_celebrities:\n",
    "    \n",
    "    def get_celebrities(self):\n",
    "        celebrities = {}\n",
    "        url = \"http://m.imdb.com/feature/bornondate\"\n",
    "        # Firefox web browser is required for Selenium to work here\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.get(url)\n",
    "        html = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        posterList = soup.findChildren('section', 'posters list')\n",
    "        iterCelebrity = iter(posterList[0].findChildren('a', 'poster '))\n",
    "\n",
    "        # go thru the list of celebrities who were born today\n",
    "        for lnk in iterCelebrity:\n",
    "            img = lnk.find('img')['src']\n",
    "            label = lnk.find('div', 'label')\n",
    "            name = label.find('span', 'title').string\n",
    "            detail = label.find('div', 'detail').string.split(',')\n",
    "            profession = detail[0]\n",
    "            bestwork = detail[1].strip()\n",
    "            celebrities[name] = (img, profession, bestwork)\n",
    "\n",
    "        return celebrities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tweetlistener(StreamListener):\n",
    "    \"\"\" a listener class for the tweet stream \"\"\"\n",
    "    \n",
    "    def on_status(self,status):\n",
    "        \"\"\"\n",
    "        when a new tweet for a celebrity comes, this\n",
    "        function is triggered\n",
    "        \"\"\"\n",
    "        global counter,Total_tweet_count,filedict\n",
    "\n",
    "        print \"----------NEW TWEET ARRIVED!-----------\"\n",
    "        print \"Tweet Text : %s\" % status.text\n",
    "        for item in filedict.items():\n",
    "            regex = item[1][2]\n",
    "            if regex.search(status.text) is not None:\n",
    "                outfile = item[1][1]\n",
    "                outfile.write(status.text)\n",
    "                outfile.write(str(\"\\n\"))\n",
    "                print \"Author's Screen name : %s\" % status.author.screen_name\n",
    "                print \"Time of creation : %s\" % status.created_at\n",
    "                print \"Source of Tweet : %s\" % status.source\n",
    "                break\n",
    "\n",
    "        counter += 1\n",
    "        # when the number of tweets reaches the limit specified\n",
    "        # by the user at the beginning of the application run,\n",
    "        # sentiment analysis starts\n",
    "        if counter >= Total_tweet_count:\n",
    "            senti1 = senti()\n",
    "            senti1.open_sentiment_files()\n",
    "            # start the analysis for the celebrities one at a time\n",
    "            for item in filedict.items():\n",
    "                senti1.sentiment_analysis(item)\n",
    "                drawing()\n",
    "                \n",
    "            # when all analysis are done, quit the program\n",
    "            sys.exit()\n",
    "\n",
    "    def on_error(self, status):\n",
    "        \"\"\" error handler during the streaming \"\"\"\n",
    "        \n",
    "        drawing()\n",
    "        print \"Too soon reconnected . Will terminate the program\"\n",
    "        print status\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sentiment analyzer class\n",
    "class senti():\n",
    "\n",
    "    def open_sentiment_files(self):\n",
    "        \"\"\" open the positive and negative words files \"\"\"\n",
    "        global positive_words,negative_words\n",
    "\n",
    "        pos_sent = open(\"positive_words.txt\").read()\n",
    "        positive_words = pos_sent.split('\\n')\n",
    "        neg_sent = open('negative_words.txt').read()\n",
    "        negative_words = neg_sent.split('\\n')\n",
    "\n",
    "    def sentiment_analysis(self, dictItem):\n",
    "        \"\"\" \n",
    "        analyze the sentiment in the tweets for each celebrity\n",
    "        The dictItem parameter is a dictionary item of which the key\n",
    "        is a celebrity's name. The value is a tuple consists of \n",
    "        (imageFile, professon, bestwork)\n",
    "        \"\"\"\n",
    "        global all_figs,positive_words,negative_words,celebrities\n",
    "\n",
    "        # key of the dictionary item\n",
    "        indiv = dictItem[0]\n",
    "        # get the matching files for the celebrity\n",
    "        file2 = dictItem[1][0]\n",
    "        outfile = dictItem[1][1]\n",
    "\n",
    "        outfile.close()\n",
    "        positive_counts = []\n",
    "        negative_counts = []\n",
    "        conclusion = []\n",
    "        tot_pos = 0\n",
    "        tot_neu = 0\n",
    "        tot_neg = 0\n",
    "        tweets = codecs.open(file2, 'r', \"utf-8\").read()\n",
    "        tweet_list_dup = []\n",
    "\n",
    "        tweets_list = tweets.split('\\n')\n",
    "\n",
    "        for tweet in tweets_list:\n",
    "            positive_counter = 0\n",
    "            negative_counter = 0\n",
    "            tweet = tweet.encode(\"utf-8\")\n",
    "            tweet_list_dup.append(tweet)\n",
    "            tweet_processed = tweet.lower()\n",
    "\n",
    "            for p in list(punctuation):\n",
    "                tweet_processed = tweet_processed.replace(p, '')\n",
    "\n",
    "            words = tweet_processed.split(' ')\n",
    "            for word in words:\n",
    "                if word in positive_words:\n",
    "                    positive_counter += 1\n",
    "                elif word in negative_words:\n",
    "                    negative_counter += 1\n",
    "\n",
    "            positive_counts.append(positive_counter)\n",
    "            negative_counts.append(negative_counter)\n",
    "            if positive_counter > negative_counter:\n",
    "                conclusion.append(\"Positive\")\n",
    "                tot_pos += 1\n",
    "            elif positive_counter == negative_counter:\n",
    "                conclusion.append(\"Neutral\")\n",
    "                tot_neu += 0.5\n",
    "            else:\n",
    "                conclusion.append(\"Negative\")\n",
    "                tot_neg +=1\n",
    "\n",
    "        output = zip(tweet_list_dup, positive_counts, negative_counts,conclusion)\n",
    "\n",
    "        print \"******** Overall Analysis **************\"\n",
    "        print \"Celebrity: \" + indiv\n",
    "        print \"Image: \" + celebrities[indiv][0]\n",
    "        print \"Profession: \" + celebrities[indiv][1]\n",
    "        print \"Best Work: \" + celebrities[indiv][2]\n",
    "\n",
    "        if tot_pos > tot_neg and tot_pos > tot_neu:\n",
    "            print \"Overall Sentiment on Twitter: POSITIVE\"\n",
    "        elif tot_neg > tot_pos and tot_neg > tot_neu:\n",
    "            print \"Overall Sentiment on Twitter: NEGATIVE\"\n",
    "        elif tot_neg == tot_neu and tot_neg > tot_pos:\n",
    "            print \"Overall Sentiment on Twitter: NEGATIVE\"\n",
    "        elif tot_pos + tot_neg < tot_neu:\n",
    "            print \"Overall Sentiment on Twitter: SEMI POSITIVE \"\n",
    "        else:\n",
    "            print \"Overall Sentiment on Twitter: NEUTRAL\"\n",
    "\n",
    "\n",
    "        print \"%%%%%%%%%%%% End of processing - \" + indiv + \"   %%%%%%%%%%%%%%%%%%%%%\"\n",
    "        print \"\"\n",
    "\n",
    "        file1 = filePath + '\\\\tweet_sentiment_' + indiv + '.csv'\n",
    "        writer = csv.writer(open(file1, 'wb'))\n",
    "        writer.writerows(output)\n",
    "        draw_helper = []\n",
    "        draw_helper.append(tot_pos)\n",
    "        draw_helper.append(tot_neg)\n",
    "        draw_helper.append(tot_neu)\n",
    "        draw_helper.append(indiv)\n",
    "        all_figs.append(draw_helper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drawing():\n",
    "    \"\"\" function used to draw a pie chart of the sentiment analysis results \"\"\"\n",
    "    \n",
    "    global all_figs, filePath\n",
    "    for one_fig in all_figs:\n",
    "        sentiments = {}\n",
    "        sentiments[\"Positive\"] = one_fig[0]\n",
    "        sentiments[\"Negative\"] = one_fig[1]\n",
    "        sentiments[\"Neutral\"] = one_fig[2]\n",
    "        all_total = one_fig[0] + one_fig[1] + one_fig[2]\n",
    "\n",
    "        sizes = [sentiments['Positive'] / float(all_total), sentiments['Negative'] / float(all_total),\n",
    "                 sentiments['Neutral'] / float(all_total)]\n",
    "\n",
    "        plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True)\n",
    "        plt.axis('equal')\n",
    "\n",
    "        plt.title('sentiment for the word - ' + str(one_fig[3]))\n",
    "        fig_name = filePath + \"\\\\fig_\" + str(one_fig[3]) + \".png\"\n",
    "        # Save the figures\n",
    "        plt.savefig(fig_name)\n",
    "        plt.close()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_tweets():\n",
    "    \"\"\" function used to pull the tweets of the celebrities \"\"\"\n",
    "    \n",
    "    global search_words_list,counter,auth,indiv,filePath,filedict\n",
    "    filePath = \"G:\\Output Files\"\n",
    "    # set up the authorization with Twitter APIs\n",
    "    consumer_key = 'ZkIxjbsPacixuhTg7aclkQ'\n",
    "    consumer_secret = 'yme0jG3UDhG0CFgqlc50UQFSspo3EkUfPziUf2FFo'\n",
    "    access_token = '1635433267-29ZpqtvpBIzVOQTnz1wgCsaotyEBTgs4V4jkUEM'\n",
    "    access_secret = '33ZEGzs7pR1M0AYnD0mwOaZJ8JIF1Nc183VOFNkeug'\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "    # create a streaming object\n",
    "    twitterStream = Stream(auth, tweetlistener())\n",
    "    counter = 0\n",
    "    filedict = {}\n",
    "    one_list = []\n",
    "    # create a list of the celebrities' name as the search keywords\n",
    "    for indiv in search_words_list:\n",
    "        filename = filePath + \"\\\\test_\" + str(indiv[0]) + \".txt\"\n",
    "        outfile = codecs.open(filename, 'w', \"utf-8\")\n",
    "        regex = re.compile(indiv)\n",
    "        # remember the file name, file handler and regex object for each celebrity\n",
    "        filedict[indiv] = (filename, outfile, regex)\n",
    "        # add each celebrity's name to the filter list\n",
    "        one_list.append(indiv)\n",
    "\n",
    "    # the one_list should have multiple celebrity names\n",
    "    twitterStream.filter(track=one_list,languages = [\"en\"], async=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\" entry point function to start the application \"\"\"\n",
    "    \n",
    "    global Total_tweet_count,search_words_list,all_figs,labels,colors,celebrities\n",
    "\n",
    "    print \"Getting celebrities...\"\n",
    "    imdb = imdb_celebrities()\n",
    "    celebrities = imdb.get_celebrities()\n",
    "    search_words_list = celebrities.keys()\n",
    "    print \"Celebrities to process: \" + \", \".join(search_words_list)\n",
    "\n",
    "    Total_tweet_count = int(raw_input(\"Enter total tweets to be pulled for all celebrities: \"))\n",
    "\n",
    "    labels = ['Positive','Negative','Neutral']\n",
    "    colors = ['yellowgreen','lightcoral','gold']\n",
    "    all_figs= []\n",
    "    search_tweets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start the application by calling the main() function\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI Challenges Faced during the project\n",
    "\n",
    "1. When working on the data strapping against the IMDB web site, I found that the data of the celebrities are created dynamically with JavaScript. In this case, the Beautisoup package by itself is not able to read the data from the web page. After doing some research, I found that the Selenium package can resolve the problem quite easily; and the code to use it is very simple. The down side of this solution is its performance may not be the best among other options. Also, it requires the installation of a web driver such as Firefox in order to work.\n",
    "\n",
    "2. Another challenge I had on the project is dealing with the connection between the application and the Tweeter APIs. Tweeter can only allow one connection at a time with its API using the same account credential. If I try to use multiple connections almost at the same time, the older connection is automatically disconnected; and the waiting time of any new connection will exponentially increase and the client IP may be even eventually banned. Because of this, I cannot create a new connection for each celebrity for which I want to stream; or try to disconnect an existing connection before creating a new one (there is no such API call to disconnect a stream other than terminating the application). The solution I came up with is to initialize one connection only. When doing the streaming, I pass all (totally 10) celebrities' name as a list to the stream filter. In this case, the stream just fetches me the tweets those match any one of the celebrities' name. Since each tweet can be for any one of the celebrities, I need to use the regular expression to find out if the text of the tweet contains the name of one of the celebrities. If it does, the tweet will be appended to corresponding file of the celebrity for later analysis and drawing. Once the streaming is done, the program will start the sentiment analysis for each celebrity and reports the results one at a time. The down side of this approach though is we need to get enough tweets from the stream, e.g. more than 1000 totally so that the tweets can be more evenly distributed among all celebrities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
